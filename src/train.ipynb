{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "Updated with NOISE_STD, VGG_WEIGHT, and library imports. Checks for GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:05:43.166675Z",
     "iopub.status.busy": "2025-12-06T12:05:43.166344Z",
     "iopub.status.idle": "2025-12-06T12:05:43.191854Z",
     "shell.execute_reply": "2025-12-06T12:05:43.191104Z",
     "shell.execute_reply.started": "2025-12-06T12:05:43.166645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.amp import autocast, GradScaler \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 100\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Device: {device}\")\n",
    "\n",
    "# Dataset Discovery\n",
    "search_path = \"/kaggle/input/**/*.pt\"\n",
    "pt_files = glob.glob(search_path, recursive=True)\n",
    "if not pt_files:\n",
    "    raise FileNotFoundError(\"âŒ Dataset not found! Make sure to upload the processed .pt file.\")\n",
    "DATA_FILE = pt_files[0]\n",
    "DATA_DIR = os.path.dirname(DATA_FILE)\n",
    "print(f\"ðŸ“‚ Dataset found at: {DATA_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class \n",
    "Added ```noise_std``` parameter for Sim-to-Real gap augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:05:53.919478Z",
     "iopub.status.busy": "2025-12-06T12:05:53.918948Z",
     "iopub.status.idle": "2025-12-06T12:05:53.927893Z",
     "shell.execute_reply": "2025-12-06T12:05:53.927269Z",
     "shell.execute_reply.started": "2025-12-06T12:05:53.919455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FluidLoader(Dataset):\n",
    "    def __init__(self, pt_file):\n",
    "        print(f\"â³ Loading data map...\")\n",
    "        try:\n",
    "            # mmap=True is crucial for Kaggle RAM limits\n",
    "            data = torch.load(pt_file, map_location='cpu', mmap=True)\n",
    "        except:\n",
    "            print(\"âš ï¸ mmap failed. Loading to RAM.\")\n",
    "            data = torch.load(pt_file, map_location='cpu')\n",
    "            \n",
    "        self.inputs = data['inputs']\n",
    "        self.targets = data['targets']\n",
    "        \n",
    "        # Load all normalization constants\n",
    "        # Fallback to 1.0 if not found (legacy support)\n",
    "        self.K_vel = float(data.get('K_vel', 1.0))\n",
    "        self.K_pres = float(data.get('K_pres', 1.0))\n",
    "        self.K_smoke = float(data.get('K_smoke', 1.0))\n",
    "            \n",
    "        print(f\"âœ… Loaded {len(self.inputs)} samples.\")\n",
    "        print(f\"   K_vel: {self.K_vel} | K_pres: {self.K_pres} | K_smoke: {self.K_smoke}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Data is already processed (Normalized & Upscaled) in the .pt file\n",
    "        # We just convert to float32 for training\n",
    "        lr = self.inputs[idx].float()\n",
    "        hr = self.targets[idx].float()\n",
    "        return lr, hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture (ResUNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:05:56.986497Z",
     "iopub.status.busy": "2025-12-06T12:05:56.986214Z",
     "iopub.status.idle": "2025-12-06T12:05:57.001710Z",
     "shell.execute_reply": "2025-12-06T12:05:57.000949Z",
     "shell.execute_reply.started": "2025-12-06T12:05:56.986475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=4, features=[64, 128, 256, 512]):\n",
    "        super(ResUNet, self).__init__()\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Encoder\n",
    "        self.input_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features[0], kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(features[0]),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        input_feat = features[0]\n",
    "        for feature in features:\n",
    "            self.encoder.append(ResidualBlock(input_feat, feature))\n",
    "            input_feat = feature\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResidualBlock(features[-1], features[-1] * 2)\n",
    "\n",
    "        # Decoder\n",
    "        self.upconvs = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "        features = features[::-1]\n",
    "        for feature in features:\n",
    "            self.upconvs.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n",
    "            self.decoder.append(ResidualBlock(feature * 2, feature))\n",
    "\n",
    "        # Output\n",
    "        self.final_conv = nn.Conv2d(features[-1], out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        out = self.input_conv(x)\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            out = layer(out)\n",
    "            skip_connections.append(out)\n",
    "            out = self.pool(out)\n",
    "            \n",
    "        out = self.bottleneck(out)\n",
    "        skip_connections = skip_connections[::-1] \n",
    "        \n",
    "        for idx in range(len(self.decoder)):\n",
    "            out = self.upconvs[idx](out)\n",
    "            skip = skip_connections[idx]\n",
    "            if out.shape != skip.shape:\n",
    "                out = F.interpolate(out, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "            concat_skip = torch.cat((skip, out), dim=1)\n",
    "            out = self.decoder[idx](concat_skip)\n",
    "            \n",
    "        out = self.final_conv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions ( VGG, Physics Informed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:06:01.086884Z",
     "iopub.status.busy": "2025-12-06T12:06:01.086092Z",
     "iopub.status.idle": "2025-12-06T12:06:01.093853Z",
     "shell.execute_reply": "2025-12-06T12:06:01.093110Z",
     "shell.execute_reply.started": "2025-12-06T12:06:01.086854Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DivergenceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DivergenceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, K_vel):\n",
    "        # Extract Velocity Channels (0, 1)\n",
    "        u = output[:, 0, :, :] * K_vel\n",
    "        v = output[:, 1, :, :] * K_vel\n",
    "        \n",
    "        # Calculate Divergence: du/dx + dv/dy\n",
    "        du_dx = u[:, :, 1:] - u[:, :, :-1]\n",
    "        dv_dy = v[:, 1:, :] - v[:, :-1, :]\n",
    "        \n",
    "        # Crop to align dimensions\n",
    "        return torch.mean((du_dx[:, :-1, :] + dv_dy[:, :, :-1])**2)\n",
    "\n",
    "class PressurePoissonLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PressurePoissonLoss, self).__init__()\n",
    "        # Laplacian kernel\n",
    "        self.register_buffer('laplacian', torch.tensor([[[[0, 1, 0], [1, -4, 1], [0, 1, 0]]]], dtype=torch.float32))\n",
    "        # Sobel kernels for gradients\n",
    "        self.register_buffer('k_x', torch.tensor([[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]], dtype=torch.float32) / 8.0)\n",
    "        self.register_buffer('k_y', torch.tensor([[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]], dtype=torch.float32) / 8.0)\n",
    "\n",
    "    def get_grads(self, f):\n",
    "        return F.conv2d(f, self.k_x, padding=1), F.conv2d(f, self.k_y, padding=1)\n",
    "\n",
    "    def get_laplacian(self, f):\n",
    "        return F.conv2d(f, self.laplacian, padding=1)\n",
    "\n",
    "    def get_divergence(self, fx, fy):\n",
    "        dfx_dx, _ = self.get_grads(fx)\n",
    "        _, dfy_dy = self.get_grads(fy)\n",
    "        return dfx_dx + dfy_dy\n",
    "\n",
    "    def forward(self, pred, K_vel, K_pres, K_smoke):\n",
    "        # Unpack and Un-normalize\n",
    "        u = pred[:, 0:1, :, :] * K_vel\n",
    "        v = pred[:, 1:2, :, :] * K_vel\n",
    "        p = pred[:, 2:3, :, :] * K_pres\n",
    "        s = pred[:, 3:4, :, :] * K_smoke\n",
    "\n",
    "        # 1. LHS: Laplacian of Pressure\n",
    "        lhs = self.get_laplacian(p)\n",
    "\n",
    "        # 2. RHS Term A: Divergence of Convection -(u.grad)u\n",
    "        du_dx, du_dy = self.get_grads(u)\n",
    "        dv_dx, dv_dy = self.get_grads(v)\n",
    "        \n",
    "        # Convection vector components\n",
    "        conv_u = u * du_dx + v * du_dy \n",
    "        conv_v = u * dv_dx + v * dv_dy \n",
    "        div_convection = self.get_divergence(conv_u, conv_v)\n",
    "\n",
    "        # 3. RHS Term B: Divergence of Buoyancy Force\n",
    "        # Force ~ (0, s * 0.3)\n",
    "        force_y = s * 0.3 \n",
    "        div_force = self.get_divergence(torch.zeros_like(s), force_y)\n",
    "\n",
    "        # Poisson Equation: Del^2 P = -Div(Conv) + Div(Force)\n",
    "        rhs = -div_convection + div_force\n",
    "\n",
    "        return F.mse_loss(lhs, rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:06:03.829416Z",
     "iopub.status.busy": "2025-12-06T12:06:03.829140Z",
     "iopub.status.idle": "2025-12-06T12:06:03.834680Z",
     "shell.execute_reply": "2025-12-06T12:06:03.833957Z",
     "shell.execute_reply.started": "2025-12-06T12:06:03.829397Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(use_physics=False, mse_weight=1.0, divergence_weight=0.1, ppe_weight=0.1):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        use_physics (bool): Whether to enable Divergence and PPE losses.\n",
    "        mse_weight (float): Weight for Pixel Loss (MSE).\n",
    "        divergence_weight (float): Weight for Divergence Loss (Mass Conservation).\n",
    "        ppe_weight (float): Weight for Pressure Poisson Loss (Navier-Stokes).\n",
    "    \"\"\"\n",
    "    # 1. Data\n",
    "    dataset = FluidLoader(DATA_FILE)\n",
    "    train_sz = int(0.8 * len(dataset))\n",
    "    val_sz = len(dataset) - train_sz\n",
    "    train_ds, val_ds = random_split(dataset, [train_sz, val_sz], generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # 2. Setup (4 Inputs -> 4 Outputs)\n",
    "    model = ResUNet(in_channels=4, out_channels=4).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    scaler = GradScaler('cuda') \n",
    "    \n",
    "    # Loss Functions\n",
    "    mse_fn = nn.MSELoss()\n",
    "    div_fn = DivergenceLoss().to(device)\n",
    "    ppe_fn = PressurePoissonLoss().to(device)\n",
    "    \n",
    "    # Normalization Constants\n",
    "    K_vel = dataset.K_vel\n",
    "    K_pres = dataset.K_pres\n",
    "    K_smoke = dataset.K_smoke\n",
    "    \n",
    "    mode_name = \"PINN_NS\" if use_physics else \"Baseline\"\n",
    "    print(f\"\\nðŸš€ Starting {mode_name} Training...\")\n",
    "    print(f\"   > Weights: MSE={mse_weight} | Div={divergence_weight} | PPE={ppe_weight}\")\n",
    "    \n",
    "    hist = {'train': [], 'val': [], 'lr': []}\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 7\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n",
    "        \n",
    "        for x, y in loop:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True) \n",
    "            \n",
    "            with autocast('cuda'): \n",
    "                pred = model(x)\n",
    "                \n",
    "                # --- Weighted Loss Calculation ---\n",
    "                loss = 0.0\n",
    "                \n",
    "                # 1. MSE Loss (Pixel Accuracy)\n",
    "                if mse_weight > 0:\n",
    "                    loss += mse_weight * mse_fn(pred, y)\n",
    "                \n",
    "                # 2. Physics Losses\n",
    "                if use_physics:\n",
    "                    # Divergence (Mass)\n",
    "                    if divergence_weight > 0:\n",
    "                        loss += divergence_weight * div_fn(pred, K_vel)\n",
    "                    \n",
    "                    # Navier-Stokes (Momentum via Pressure)\n",
    "                    if ppe_weight > 0:\n",
    "                        loss += ppe_weight * ppe_fn(pred, K_vel, K_pres, K_smoke)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with autocast('cuda'):\n",
    "                    pred = model(x)\n",
    "                    # Track MSE for early stopping\n",
    "                    val_loss += mse_fn(pred, y).item()\n",
    "        \n",
    "        avg_train = train_loss / len(train_loader)\n",
    "        avg_val = val_loss / len(val_loader)\n",
    "        \n",
    "        scheduler.step(avg_val)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        hist['train'].append(avg_train)\n",
    "        hist['val'].append(avg_val)\n",
    "        hist['lr'].append(current_lr)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Train: {avg_train:.6f} | Val: {avg_val:.6f} | LR: {current_lr:.2e}\")\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_val,\n",
    "        }\n",
    "        \n",
    "        if avg_val < best_val_loss:\n",
    "            best_val_loss = avg_val\n",
    "            counter = 0\n",
    "            torch.save(checkpoint, f\"{OUTPUT_DIR}/{mode_name}_best.pth\")\n",
    "            print(\"  --> New Best Model Saved!\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"ðŸ›‘ Early Stopping triggered.\")\n",
    "                break\n",
    "                \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(hist['train'], label='Train')\n",
    "    plt.plot(hist['val'], label='Val')\n",
    "    plt.title(f'{mode_name} Loss')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(hist['lr'], color='orange')\n",
    "    plt.title('Learning Rate')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.savefig(f\"{OUTPUT_DIR}/{mode_name}_history.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    best_checkpoint = torch.load(f\"{OUTPUT_DIR}/{mode_name}_best.pth\")\n",
    "    model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute\n",
    "Run this cell to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-06T12:06:12.555688Z",
     "iopub.status.busy": "2025-12-06T12:06:12.555079Z",
     "iopub.status.idle": "2025-12-06T14:21:52.384643Z",
     "shell.execute_reply": "2025-12-06T14:21:52.383999Z",
     "shell.execute_reply.started": "2025-12-06T12:06:12.555667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train with Navier-Stokes physics\n",
    "model, ds = train(\n",
    "    use_physics=True, \n",
    "    mse_weight=1.0, \n",
    "    divergence_weight=0.1, \n",
    "    ppe_weight=0.1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8927398,
     "sourceId": 14013478,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

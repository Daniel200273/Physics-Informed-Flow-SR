{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T09:57:09.704317Z",
     "iopub.status.busy": "2025-12-15T09:57:09.703976Z",
     "iopub.status.idle": "2025-12-15T09:57:09.717763Z",
     "shell.execute_reply": "2025-12-15T09:57:09.717085Z",
     "shell.execute_reply.started": "2025-12-15T09:57:09.704292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Fine-Tuning Configuration ---\n",
    "BATCH_SIZE = 4          \n",
    "LEARNING_RATE_G = 1e-5  # CRITICAL: 10x smaller than pre-training\n",
    "LEARNING_RATE_D = 1e-5  # CRITICAL: 10x smaller\n",
    "EPOCHS = 20             # Shorter training for adaptation\n",
    "PRETRAIN_EPOCHS = 0     # START GAN IMMEDIATELY (Discriminator active from Epoch 1)\n",
    "\n",
    "# Weights (High Physics Weight for Realism)\n",
    "MSE_WEIGHT = 1.0        \n",
    "PHYSICS_WEIGHT = 0.1    \n",
    "ADVERSARIAL_WEIGHT = 0.001 \n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üöÄ Device: {DEVICE}\")\n",
    "\n",
    "# --- Dataset: Native 64x64 (Noisy/Sim-to-Real) ---\n",
    "# Update this path if your dataset name is different\n",
    "DATA_FILE = \"/kaggle/input/navier-stokes-dataset/native_64.pt\" \n",
    "print(f\"üìÇ Fine-Tuning Dataset: {DATA_FILE}\")\n",
    "\n",
    "# --- Checkpoints: Pre-trained Weights ---\n",
    "# Assuming you uploaded your previous output as a dataset named 'checkpoints'\n",
    "PRETRAINED_G = \"/kaggle/input/checkpoints/SRGAN_Gen_epoch_100.pth\"\n",
    "PRETRAINED_D = \"/kaggle/input/checkpoints/SRGAN_Disc_epoch_100.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class \n",
    "Added ```noise_std``` parameter for Sim-to-Real gap augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T09:57:09.719411Z",
     "iopub.status.busy": "2025-12-15T09:57:09.719111Z",
     "iopub.status.idle": "2025-12-15T09:57:09.733125Z",
     "shell.execute_reply": "2025-12-15T09:57:09.732578Z",
     "shell.execute_reply.started": "2025-12-15T09:57:09.719394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FluidLoader(Dataset):\n",
    "    def __init__(self, pt_file):\n",
    "        print(f\"‚è≥ Loading data from {pt_file}...\")\n",
    "        try:\n",
    "            data = torch.load(pt_file, map_location='cpu', mmap=True)\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è mmap failed. Loading entire dataset to RAM.\")\n",
    "            data = torch.load(pt_file, map_location='cpu')\n",
    "            \n",
    "        self.inputs = data['inputs']\n",
    "        self.targets = data['targets']\n",
    "        \n",
    "        self.K_vel = float(data.get('K_vel', 1.0))\n",
    "        self.K_pres = float(data.get('K_pres', 1.0))\n",
    "        self.K_smoke = float(data.get('K_smoke', 1.0))\n",
    "            \n",
    "        print(f\"‚úÖ Loaded {len(self.inputs)} samples.\")\n",
    "        print(f\"   Normalization: v={self.K_vel}, p={self.K_pres}, s={self.K_smoke}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr = self.inputs[idx].float()\n",
    "        hr = self.targets[idx].float()\n",
    "        \n",
    "        # --- CRITICAL FIX: Revert Upscaled Inputs ---\n",
    "        # If the input is 256x256, downscale it to 64x64 so the Generator accepts it.\n",
    "        if lr.shape[-1] == 256:\n",
    "            lr = F.interpolate(lr.unsqueeze(0), size=(64, 64), mode='bilinear', align_corners=False).squeeze(0)\n",
    "            \n",
    "        return lr, hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture (SRGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T09:57:09.734250Z",
     "iopub.status.busy": "2025-12-15T09:57:09.733923Z",
     "iopub.status.idle": "2025-12-15T09:57:09.755082Z",
     "shell.execute_reply": "2025-12-15T09:57:09.754476Z",
     "shell.execute_reply.started": "2025-12-15T09:57:09.734224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Generator (SRResNet) ---\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.prelu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return out + residual\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, scale_factor):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels * (scale_factor ** 2), kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.prelu(self.pixel_shuffle(self.conv(x)))\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=4, hidden_channels=64, num_res_blocks=16):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels, hidden_channels, kernel_size=9, padding=4), nn.PReLU())\n",
    "        res_blocks = [ResidualBlock(hidden_channels) for _ in range(num_res_blocks)]\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, padding=1, bias=False), nn.BatchNorm2d(hidden_channels))\n",
    "        self.upsample = nn.Sequential(UpsampleBlock(hidden_channels, 2), UpsampleBlock(hidden_channels, 2))\n",
    "        self.final_conv = nn.Conv2d(hidden_channels, out_channels, kernel_size=9, padding=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.conv1(x)\n",
    "        out = self.res_blocks(out1)\n",
    "        out = self.conv2(out)\n",
    "        out = out + out1 \n",
    "        out = self.upsample(out)\n",
    "        out = self.final_conv(out)\n",
    "        return out\n",
    "\n",
    "# --- Discriminator ---\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=4, hidden_channels=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        def conv_block(in_c, out_c, stride=1, bn=True):\n",
    "            layers = [nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)]\n",
    "            if bn: layers.append(nn.BatchNorm2d(out_c))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            *conv_block(in_channels, hidden_channels, stride=1, bn=False),\n",
    "            *conv_block(hidden_channels, hidden_channels, stride=2, bn=True),\n",
    "            *conv_block(hidden_channels, hidden_channels*2, stride=1, bn=True),\n",
    "            *conv_block(hidden_channels*2, hidden_channels*2, stride=2, bn=True),\n",
    "            *conv_block(hidden_channels*2, hidden_channels*4, stride=1, bn=True),\n",
    "            *conv_block(hidden_channels*4, hidden_channels*4, stride=2, bn=True),\n",
    "            *conv_block(hidden_channels*4, hidden_channels*8, stride=1, bn=True),\n",
    "            *conv_block(hidden_channels*8, hidden_channels*8, stride=2, bn=True),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(),\n",
    "            nn.Linear(hidden_channels*8, 1024), nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions ( Poisson Pressure equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T09:57:09.756116Z",
     "iopub.status.busy": "2025-12-15T09:57:09.755877Z",
     "iopub.status.idle": "2025-12-15T09:57:09.771749Z",
     "shell.execute_reply": "2025-12-15T09:57:09.771197Z",
     "shell.execute_reply.started": "2025-12-15T09:57:09.756096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PressurePoissonLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PressurePoissonLoss, self).__init__()\n",
    "        self.register_buffer('laplacian', torch.tensor([[[[0, 1, 0], [1, -4, 1], [0, 1, 0]]]], dtype=torch.float32))\n",
    "        self.register_buffer('k_x', torch.tensor([[[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]]], dtype=torch.float32) / 8.0)\n",
    "        self.register_buffer('k_y', torch.tensor([[[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]]], dtype=torch.float32) / 8.0)\n",
    "\n",
    "    def get_grads(self, f): return F.conv2d(f, self.k_x, padding=1), F.conv2d(f, self.k_y, padding=1)\n",
    "    def get_laplacian(self, f): return F.conv2d(f, self.laplacian, padding=1)\n",
    "    def get_divergence(self, fx, fy): return self.get_grads(fx)[0] + self.get_grads(fy)[1]\n",
    "\n",
    "    def forward(self, pred, K_vel, K_pres, K_smoke):\n",
    "        u, v, p, s = pred[:,0:1]*K_vel, pred[:,1:2]*K_vel, pred[:,2:3]*K_pres, pred[:,3:4]*K_smoke\n",
    "        lhs = self.get_laplacian(p)\n",
    "        du_dx, du_dy = self.get_grads(u)\n",
    "        dv_dx, dv_dy = self.get_grads(v)\n",
    "        div_conv = self.get_divergence(u*du_dx + v*du_dy, u*dv_dx + v*dv_dy)\n",
    "        div_force = self.get_divergence(torch.zeros_like(s), s*0.3)\n",
    "        return F.mse_loss(lhs, -div_conv + div_force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T09:57:09.955997Z",
     "iopub.status.busy": "2025-12-15T09:57:09.955469Z",
     "iopub.status.idle": "2025-12-15T09:57:09.968689Z",
     "shell.execute_reply": "2025-12-15T09:57:09.968063Z",
     "shell.execute_reply.started": "2025-12-15T09:57:09.955975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def train_finetune():\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "    # 1. Dataset\n",
    "    dataset = FluidLoader(DATA_FILE)\n",
    "    train_sz = int(0.9 * len(dataset))\n",
    "    val_sz = len(dataset) - train_sz\n",
    "    train_ds, val_ds = random_split(dataset, [train_sz, val_sz], generator=torch.Generator().manual_seed(42))\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # 2. Models\n",
    "    netG = Generator(in_channels=4, out_channels=4).to(DEVICE)\n",
    "    netD = Discriminator(in_channels=4).to(DEVICE)\n",
    "\n",
    "    # --- LOAD PRE-TRAINED WEIGHTS ---\n",
    "    print(f\"\\nüì• Loading Pre-trained Weights...\")\n",
    "    if os.path.exists(PRETRAINED_G):\n",
    "        ckpt_g = torch.load(PRETRAINED_G, map_location=DEVICE)\n",
    "        # Handle dict vs raw state_dict\n",
    "        if 'model_state_dict' in ckpt_g: ckpt_g = ckpt_g['model_state_dict']\n",
    "        netG.load_state_dict(ckpt_g)\n",
    "        print(\"   ‚úÖ Generator Loaded\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Generator weights NOT FOUND at {PRETRAINED_G}!\")\n",
    "\n",
    "    if os.path.exists(PRETRAINED_D):\n",
    "        ckpt_d = torch.load(PRETRAINED_D, map_location=DEVICE)\n",
    "        if 'model_state_dict' in ckpt_d: ckpt_d = ckpt_d['model_state_dict']\n",
    "        netD.load_state_dict(ckpt_d)\n",
    "        print(\"   ‚úÖ Discriminator Loaded\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Discriminator weights not found. Re-initializing D.\")\n",
    "\n",
    "    # 3. Optimizers (Low LR)\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=LEARNING_RATE_G)\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=LEARNING_RATE_D)\n",
    "\n",
    "    # 4. Losses\n",
    "    mse_fn = nn.MSELoss()\n",
    "    gan_fn = nn.BCEWithLogitsLoss()\n",
    "    phys_fn = PressurePoissonLoss().to(DEVICE)\n",
    "    scaler = GradScaler('cuda')\n",
    "    K_vel, K_pres, K_smoke = dataset.K_vel, dataset.K_pres, dataset.K_smoke\n",
    "\n",
    "    print(f\"\\nüöÄ Starting Fine-Tuning (Epochs={EPOCHS}, LR={LEARNING_RATE_G})...\")\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        netG.train(); netD.train()\n",
    "        loss_g_accum = 0.0; loss_d_accum = 0.0\n",
    "        \n",
    "        loop = tqdm(train_loader, desc=f\"Fine-Tune {epoch+1}/{EPOCHS}\", leave=False)\n",
    "        \n",
    "        for lr_img, hr_img in loop:\n",
    "            lr_img, hr_img = lr_img.to(DEVICE), hr_img.to(DEVICE)\n",
    "            \n",
    "            # Update G\n",
    "            optimizerG.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                sr_img = netG(lr_img)\n",
    "                loss_content = MSE_WEIGHT * mse_fn(sr_img, hr_img)\n",
    "                loss_phys = PHYSICS_WEIGHT * phys_fn(sr_img, K_vel, K_pres, K_smoke)\n",
    "                pred_fake = netD(sr_img)\n",
    "                loss_adv = ADVERSARIAL_WEIGHT * gan_fn(pred_fake, torch.ones_like(pred_fake))\n",
    "                loss_G = loss_content + loss_phys + loss_adv\n",
    "            \n",
    "            scaler.scale(loss_G).backward()\n",
    "            scaler.step(optimizerG)\n",
    "            loss_g_accum += loss_G.item()\n",
    "\n",
    "            # Update D\n",
    "            optimizerD.zero_grad()\n",
    "            with autocast('cuda'):\n",
    "                pred_real = netD(hr_img)\n",
    "                loss_real = gan_fn(pred_real, torch.ones_like(pred_real))\n",
    "                pred_fake = netD(sr_img.detach())\n",
    "                loss_fake = gan_fn(pred_fake, torch.zeros_like(pred_fake))\n",
    "                loss_D = (loss_real + loss_fake) / 2\n",
    "            \n",
    "            scaler.scale(loss_D).backward()\n",
    "            scaler.step(optimizerD)\n",
    "            scaler.update()\n",
    "            loss_d_accum += loss_D.item()\n",
    "            \n",
    "            loop.set_postfix(G=loss_G.item(), D=loss_D.item())\n",
    "\n",
    "        # Validation\n",
    "        avg_g = loss_g_accum / len(train_loader)\n",
    "        netG.eval()\n",
    "        val_mse = 0.0\n",
    "        with torch.no_grad():\n",
    "            for lr, hr in val_loader:\n",
    "                lr, hr = lr.to(DEVICE), hr.to(DEVICE)\n",
    "                with autocast('cuda'):\n",
    "                    val_mse += mse_fn(netG(lr), hr).item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | G_Loss: {avg_g:.5f} | Val MSE: {val_mse/len(val_loader):.6f}\")\n",
    "\n",
    "        # Save Checkpoints\n",
    "        if (epoch+1) % 5 == 0 or epoch == EPOCHS-1:\n",
    "            os.makedirs(\"checkpoints_finetuned\", exist_ok=True)\n",
    "            torch.save(netG.state_dict(), f\"checkpoints_finetuned/SRGAN_FT_Gen_epoch_{epoch+1}.pth\")\n",
    "            torch.save(netD.state_dict(), f\"checkpoints_finetuned/SRGAN_FT_Disc_epoch_{epoch+1}.pth\")\n",
    "\n",
    "    print(\"‚úÖ Fine-Tuning Complete.\")\n",
    "    return netG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute\n",
    "Run this cell to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run the training pipeline\n",
    "model = train_finetune()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 9011425,
     "sourceId": 14140569,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9063932,
     "sourceId": 14210131,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
